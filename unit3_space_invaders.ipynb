{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d72bc638",
   "metadata": {},
   "source": [
    "## 0. Google colab extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6babb5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20280b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"Is CUDA available? {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Current GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d53947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we are running in Colab\n",
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    !pip install \"gymnasium[atari,accept-rom-license]\"\n",
    "    !pip install shimmy\n",
    "\n",
    "    !pip install torch torchvision torchaudio\n",
    "    # Add any other specific libs from your local venv\n",
    "\n",
    "\n",
    "    # Install the engine and the gymnasium atari support\n",
    "\n",
    "# These two lines are often the \"missing piece\" on Colab to register the ALE namespace\n",
    "import shimmy\n",
    "import ale_py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90975fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "virtual_mem = psutil.virtual_memory()\n",
    "print(f\"Available RAM: {virtual_mem.available / (1024**3):.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc683ca",
   "metadata": {},
   "source": [
    "## 1. Helper modules and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a4e2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque, namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477629dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is my 'rational brain'\n",
    "class QNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, action_size):\n",
    "        super(QNetwork, self).__init__()\n",
    "\n",
    "        # Input: (4, 84, 84) ---- 4 stacked grayscale frames\n",
    "        self.conv = nn.Sequential(\n",
    "            # converlutional layer (input-channels, output-channels, kernals, shrinkage), activation\n",
    "            # output  = ( (inputs - kernal_size ) / stride ) + 1\n",
    "            nn.Conv2d(4, 32,  kernel_size=8, stride=4), nn.ReLU(), # ( 84 - 8 ) / 4 + 1 = 20    \n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2), nn.ReLU(), # ( 20 - 4 ) / 2 + 1 = 9\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1), nn.ReLU() # ( 9  - 3 ) / 1 + 1 = 7\n",
    "        ) # return the (channel, width, height) -> (64, 7, 7) \n",
    "\n",
    "        # fully connected layer\n",
    "        # 64 x 7 x 7 = 3136\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(3136, 512), nn.ReLU(),\n",
    "            nn.Linear(512, action_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1) # flatten 64 x 7 x 7\n",
    "        return self.fc(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701c3277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deinfe the structure of one single memory\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward', 'done'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3c45c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory, here is my \"emotional brain\"\n",
    "class ReplayBuffer:\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.memory, batch_size)  # ATTENTION: RANDOM! Even though some memories might contribute a tiny to current state\n",
    "        states, actions,next_states, rewards, dones = zip(*batch) \n",
    "        return (np.stack(states), \n",
    "                np.array(actions),\n",
    "                np.array(rewards, dtype=np.float32),\n",
    "                np.stack(next_states),\n",
    "                np.array(dones, np.uint8) # 0: continue  1: game over\n",
    "               )\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34fc0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The epsilon-greedy policy\n",
    "# A modification of 'epsilon_greedy()' in unit2_forzen_lake_and_taxi.py\n",
    "# ATTENTION: one state -> one decision\n",
    "def select_action(policy_net, state, epsilon, device, action_space):\n",
    "\n",
    "    # prepare the state\n",
    "    # NN works with float -> change the pixels from 0-255 to 0.0-1.0\n",
    "    state_v = torch.FloatTensor(np.array(state)).unsqueeze(0).to(device) / 255.0\n",
    "\n",
    "    if random.random() > epsilon: \n",
    "        # disables gradient calculation to run faster, thanks gemini.\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state_v).argmax().item() # I am sitting at the 1-epsilon area: exploitation\n",
    "        \n",
    "    else: # I am sitting at the epsilon area: exploration\n",
    "        # actually, to get the action size, \n",
    "        return action_space.sample() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffab896",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model(memory, policy_net, target_net, optimizer, batch_size, gamma, device):\n",
    "    \n",
    "    if len(memory) < batch_size:\n",
    "        return # Not enough memories to learn yet!\n",
    "    \n",
    "    # 1. sample a 'random' transition from the memory\n",
    "    # transitions = memory.sample(batch_size)  # ATTENTION: RANDOM! Even though some memories might contribute a tiny to current state\n",
    "    #batch = Transition(*zip(*transitions)) # do not have to unzip it again\n",
    "    b_state, b_action, b_reward, b_next_state, b_done = memory.sample(batch_size)\n",
    "    \n",
    "    # 2. convert raw data into PyTorch Tensors\n",
    "    state_batch  = torch.FloatTensor(np.array(b_state)).to(device) / 255.0 \n",
    "    action_batch = torch.LongTensor(b_action).unsqueeze(1).to(device) # to a column\n",
    "    reward_batch = torch.FloatTensor(b_reward).to(device)\n",
    "    next_state_batch = torch.FloatTensor(np.array(b_next_state)).to(device) / 255.0 \n",
    "    done_batch   = torch.FloatTensor(b_done).to(device) # targrt = reward + (future * (1-done))\n",
    "\n",
    "    # 3. Loss \n",
    "    # 3.1 current q-value (prediction)\n",
    "    current_q_value = policy_net(state_batch).gather(1, action_batch) # 1: column (action)\n",
    "\n",
    "    # 3.2 next \"best possible\" Target value  \n",
    "    with torch.no_grad():\n",
    "        max_next_q_value = target_net(next_state_batch).max(1)[0] # max(1): best posiible [0]: value\n",
    "    expected_q_value = reward_batch + (gamma * max_next_q_value * (1-done_batch)) # \" TD target \"\n",
    "\n",
    "    loss = F.mse_loss(current_q_value.squeeze(), expected_q_value)\n",
    "\n",
    "    # Finally, comes to the \"OPTIMIZATION\"\n",
    "    # based on the loss, trace back and update the weight\n",
    "    optimizer.zero_grad() # keep current 32 memories \n",
    "    loss.backward() # back-ptopagation to get the \"Gradient\" (local minimum)\n",
    "    # Hmm, First order Taylor approximation !!!  -> Flat world assumption !!! \n",
    "    torch.nn.utils.clip_grad_norm_(policy_net.parameters(), 1)\n",
    "    optimizer.step() # update "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943b4f1a",
   "metadata": {},
   "source": [
    "## 2. set the env and load policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a6c9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import AtariPreprocessing\n",
    "#from gymnasium.wrappers import FrameStack\n",
    "from gymnasium.wrappers import FrameStackObservation\n",
    "#from unit3_helper import QNetwork, ReplayBuffer, select_action, optimize_model # Import NN helpers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f73ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env setup\n",
    "env = gym.make(\"ALE/SpaceInvaders-v5\", frameskip=1, render_mode='rgb_array') # with NO internal skipping\n",
    "env = AtariPreprocessing(env, screen_size=84, grayscale_obs=True, frame_skip=4)\n",
    "env = FrameStackObservation(env, stack_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285dc2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the policy from helper function\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Hmm, Meine CPU\n",
    "policy_net = QNetwork(env.action_space.n).to(device) # learning...\n",
    "target_net = QNetwork(env.action_space.n).to(device) # frozen snapshot for stability (like a notebook)\n",
    "target_net.load_state_dict(policy_net.state_dict())  # sync\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=1e-4) # optimize the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8ee0d0",
   "metadata": {},
   "source": [
    "## 3. set parameters and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4e065e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each episode, load the memory\n",
    "n_episodes = 100\n",
    "nsteps = 10000\n",
    "batch_size = 64\n",
    "gamma = 0.99\n",
    "target_update = 2000\n",
    "step_done = 0\n",
    "eps_end = 0.01\n",
    "eps_start = 1.0\n",
    "eps_decay = 50000 \n",
    "opt_step = 4\n",
    "\n",
    "memory = ReplayBuffer(30000) # do not have that much ram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ad495f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(n_episodes):\n",
    "    \n",
    "    state, _ = env.reset()\n",
    "    # no memory will be discard through episode, so move it outside the loop\n",
    "    # memory = ReplayBuffer(10000, state, action, reward, next_state, done)\n",
    "    episode_reward = 0\n",
    "    for t in range(nsteps):\n",
    "        # which action   \n",
    "        epsilon = max(eps_end, eps_start - (step_done / eps_decay))\n",
    "        action = select_action(policy_net, state, epsilon, device, env.action_space)\n",
    "        # action\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        # memorize\n",
    "        memory.push(state, action, next_state, reward, done)\n",
    "        # update\n",
    "        state = next_state\n",
    "        step_done += 1\n",
    "        # train\n",
    "        if len(memory) > batch_size and step_done % opt_step == 0: # optimize every 4 steps\n",
    "            optimize_model(memory, policy_net, target_net, optimizer, batch_size, gamma, device)\n",
    "\n",
    "        if step_done % target_update == 0: \n",
    "            target_net.load_state_dict(policy_net.state_dict()) # update / buffer to target net\n",
    "            print(f\"Target Network Updated at step {step_done} with epsilon {epsilon}\")\n",
    "        \n",
    "        episode_reward += reward\n",
    "\n",
    "        if done: \n",
    "            break\n",
    "    print(f\"ID: {episode} | Score: {episode_reward:.1f} | Steps: {t} | Eps: {epsilon:.2f}\")\n",
    "    \n",
    "    # save \n",
    "    if episode % 10 == 0:\n",
    "        torch.save(policy_net.state_dict(), \"space_invaders_model.pth\")\n",
    "        print(\">>> Checkpoint Saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265dec57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not forget to free up ram\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f178dff",
   "metadata": {},
   "source": [
    "## 4. evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ae67b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RecordVideo, AtariPreprocessing, FrameStackObservation\n",
    "import torch\n",
    "import numpy as np\n",
    "from IPython.display import Video\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e3d7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Set frameskip=1 here to disable internal skipping\n",
    "env = gym.make(\"ALE/SpaceInvaders-v5\", frameskip=1, render_mode=\"rgb_array\")\n",
    "\n",
    "# 2. Now the wrapper is allowed to handle the frame_skip=4\n",
    "env = AtariPreprocessing(env, screen_size=84, grayscale_obs=True, frame_skip=4)\n",
    "\n",
    "# 3. Stack the observations\n",
    "env = FrameStackObservation(env, stack_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382295f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will save the mp4 to the 'videos' folder\n",
    "env = RecordVideo(env, video_folder=\"./videos\", episode_trigger=lambda x: True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9398152d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Run the \"Final Exam\"\n",
    "state, _ = env.reset()\n",
    "policy_net.eval() # Set to evaluation mode\n",
    "total_reward = 0\n",
    "done = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a266dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "while not done:\n",
    "    # Convert state to tensor for the 'Rational Brain'\n",
    "    state_v = torch.FloatTensor(np.array(state)).unsqueeze(0).to(device) / 255.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Pick the best action (No epsilon randomness here!)\n",
    "        action = policy_net(state_v).argmax().item()\n",
    "    \n",
    "    state, reward, terminated, truncated, _ = env.step(action)\n",
    "    total_reward += reward\n",
    "    done = terminated or truncated\n",
    "env.close()\n",
    "print(f\"Final Evaluation Score: {total_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7aa249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Show the video in your VS Code/Colab Notebook\n",
    "video_files = glob.glob(\"./videos/*.mp4\")\n",
    "if video_files:\n",
    "    # Display the most recent video\n",
    "    latest_video = max(video_files, key=os.path.getctime)\n",
    "    display(Video(latest_video, embed=True, width=500))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032b09f3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
